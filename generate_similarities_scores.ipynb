{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x74253a0ff450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import json\n",
    "from datasets import load_metric\n",
    "\n",
    "# Please make sure you are using CUDA enabled GPU for this project\n",
    "device = 'cuda'\n",
    "\n",
    "# Setting the seed value ensures that the results are reproducible across different runs\n",
    "seed_val = 10\n",
    "\n",
    "# Ensuring that the seed is set for Python's hashing, random operations, NumPy, and PyTorch\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Tokenizer for the given model\n",
    "# Since we will be using the same tokenizer for other notebooks, we will save it in the cache directory\n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "\n",
    "# Using Microsoft DeBERTa model for the generation of similarities\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-large-mnli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-large-mnli\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = json.loads(open(\"data/activations/generations_and_judgments_with_sae_20.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/mpapucci/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Tue Mar  4 17:22:17 2025) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n",
      "/tmp/ipykernel_3407/4269451919.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  meteor = load_metric('meteor')\n",
      "[nltk_data] Downloading package wordnet to /home/mpapucci/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mpapucci/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mpapucci/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Using Rouge to evaluate syntactic similarity for our datasets (coQA)\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "# METEOR metric can be used for evaluating summarization tasks (useful for some time of datasets but not for ours)\n",
    "meteor = load_metric('meteor')\n",
    "\n",
    "deberta_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 817/817 [04:22<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for sample_idx in tqdm(responses):\n",
    "    sample = responses[sample_idx]\n",
    "    # Getting data from the sample\n",
    "    # Please run the cleaner notebook before running this code\n",
    "    question_text = sample['prompt']\n",
    "    generated_texts = sample['generations']\n",
    "    sample_id = sample_idx\n",
    "    unique_responses = list(set(generated_texts))\n",
    "\n",
    "    # Variables for semantic similarity analysis\n",
    "    answer_pairs_1 = []\n",
    "    answer_pairs_2 = []\n",
    "    has_semantically_different_answers = False\n",
    "    syntactic_similarities = {rouge_type: 0.0 for rouge_type in ['rouge1', 'rouge2', 'rougeL']}\n",
    "    semantic_set_ids = {answer: index for index, answer in enumerate(unique_responses)}\n",
    "\n",
    "    # print('No of unique answers:', len(unique_responses))\n",
    "    per_iter_deberta_predictions = []\n",
    "    # Evalauate semantic similarity if there are more than 1 unique answers\n",
    "    if len(unique_responses) > 1:\n",
    "        \n",
    "        for i, reference_answer in enumerate(unique_responses):\n",
    "            for j in range(i + 1, len(unique_responses)):\n",
    "\n",
    "                answer_pairs_1.append(unique_responses[i])\n",
    "                answer_pairs_2.append(unique_responses[j])\n",
    "\n",
    "                # Create input pairs and encode them\n",
    "                input_pair = question_text + ' ' + unique_responses[i] + ' [SEP] ' + unique_responses[j]\n",
    "                encoded_input = tokenizer.encode(input_pair, padding=True)\n",
    "                prediction = model(torch.tensor([encoded_input], device='cuda'))['logits']\n",
    "                predicted_label = torch.argmax(prediction, dim=1)\n",
    "\n",
    "                # Reverse the input pair and encode\n",
    "                reverse_input_pair = question_text + ' ' + unique_responses[j] + ' [SEP] ' + unique_responses[i]\n",
    "                encoded_reverse_input = tokenizer.encode(reverse_input_pair, padding=True)\n",
    "                reverse_prediction = model(torch.tensor([encoded_reverse_input], device='cuda'))['logits']\n",
    "                reverse_predicted_label = torch.argmax(reverse_prediction, dim=1)\n",
    "\n",
    "                # Determine semantic similarity\n",
    "                deberta_prediction = 0 if 0 in predicted_label or 0 in reverse_predicted_label else 1\n",
    "                if deberta_prediction == 0:\n",
    "                    has_semantically_different_answers = True\n",
    "                else:\n",
    "                    semantic_set_ids[unique_responses[j]] = semantic_set_ids[unique_responses[i]]\n",
    "\n",
    "                deberta_predictions.append([unique_responses[i], unique_responses[j], deberta_prediction])\n",
    "                per_iter_deberta_predictions.append([i, j, deberta_prediction])\n",
    "\n",
    "        # Evalauate syntactic similarity\n",
    "        results = rouge_metric.compute(predictions=answer_pairs_1, references=answer_pairs_2)\n",
    "        for rouge_type in syntactic_similarities.keys():\n",
    "            syntactic_similarities[rouge_type] = results[rouge_type]\n",
    "    \n",
    "    # Store the results in the result dictionary\n",
    "    result_dict[sample_id] = {\n",
    "        'syntactic_similarities': syntactic_similarities,\n",
    "        'has_semantically_different_answers': has_semantically_different_answers,\n",
    "        'semantic_set_ids': [semantic_set_ids[x] for x in generated_texts],\n",
    "        'deberta_predictions': per_iter_deberta_predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "with open('data/activations/deberta_predictions_20.csv', \"w\", encoding=\"UTF8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Writing header row and deberta predictions for the CSV file\n",
    "    writer.writerow(['qa_1', 'qa_2', 'prediction'])\n",
    "    writer.writerows(deberta_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8170"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deberta_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('data/activations/similarity_scores_20.json', \"w\") as f:\n",
    "    f.write(json.dumps(result_dict))\n",
    "with open(f'data/activations/similarity_scores_20.pkl','wb') as outfile:\n",
    "    pickle.dump(result_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
