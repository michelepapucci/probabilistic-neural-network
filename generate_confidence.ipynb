{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Please make sure you are using CUDA enabled GPU for this project\n",
    "device = 'cpu'\n",
    "\n",
    "# Setting the seed value ensures that the results are reproducible across different runs\n",
    "seed_val = 10\n",
    "\n",
    "# Ensuring that the seed is set for Python's hashing, random operations, NumPy, and PyTorch\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "llh_shift = torch.tensor(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood_metrics(results_list):\n",
    "    \"\"\"\n",
    "    Compute log likelihood and related metrics for all generations under their given context.\n",
    "    \n",
    "    results_list: list of tuples with model size and result dictionary\n",
    "    \n",
    "    returns: dictionary with keys such as 'neg_log_likelihoods', 'average_neg_log_likelihoods', etc.\n",
    "             containing tensors of shape (num_models, num_generations, num_samples_per_generation)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a dictionary to store the results\n",
    "    metrics_dict = {}\n",
    "\n",
    "    # Define the keys that need to be processed\n",
    "    keys_to_process = ['neg_log_likelihoods', 'average_neg_log_likelihoods', 'sequence_embeddings',\n",
    "                       'pointwise_mutual_information', 'average_neg_log_likelihood_of_most_likely_gen',\n",
    "                       'average_neg_log_likelihood_of_second_most_likely_gen', 'neg_log_likelihood_of_most_likely_gen', 'semantic_set_ids']\n",
    "\n",
    "    # Iterate through the keys and process the results\n",
    "    for key in keys_to_process:\n",
    "        ids_list = []\n",
    "        combined_results = []\n",
    "        for model_size, result in results_list:\n",
    "            results_for_model = []\n",
    "            for sample in result:\n",
    "                avg_neg_log_likelihoods = sample[key]\n",
    "                ids_list.append(sample['id'])\n",
    "                results_for_model.append(avg_neg_log_likelihoods)\n",
    "\n",
    "            results_for_model = torch.stack(results_for_model)\n",
    "            combined_results.append(results_for_model)\n",
    "\n",
    "        # Stack the results if the key is not 'sequence_embeddings'\n",
    "        if key != 'sequence_embeddings':\n",
    "            combined_results = torch.stack(combined_results)\n",
    "\n",
    "        metrics_dict[key] = combined_results\n",
    "\n",
    "    # Add the list of IDs to the result dictionary\n",
    "    metrics_dict['id'] = ids_list\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def compute_mutual_information(log_likelihoods):\n",
    "    \"\"\"Compute a confidence measure for a given set of likelihoods using mutual information.\"\"\"\n",
    "    \n",
    "    num_models = torch.tensor(log_likelihoods.shape[0])\n",
    "    mean_across_models = torch.logsumexp(log_likelihoods, dim=0) - torch.log(num_models)\n",
    "    tiled_mean = mean_across_models.tile(num_models, 1, 1)\n",
    "    diff_term = torch.exp(log_likelihoods) * log_likelihoods - torch.exp(tiled_mean) * tiled_mean\n",
    "    f_j = torch.div(torch.sum(diff_term, dim=0), diff_term.shape[0])\n",
    "    mutual_information = torch.div(torch.sum(torch.div(f_j, mean_across_models), dim=1), f_j.shape[-1])\n",
    "\n",
    "    return mutual_information\n",
    "\n",
    "def compute_log_likelihood_variance(neg_log_likelihoods):\n",
    "    \"\"\"Compute the variance of negative log likelihoods for approximate posterior predictive.\"\"\"\n",
    "    \n",
    "    mean_across_models = torch.mean(neg_log_likelihoods, dim=0)\n",
    "    variance_of_neg_log_likelihoods = torch.var(mean_across_models, dim=1)\n",
    "\n",
    "    return variance_of_neg_log_likelihoods\n",
    "\n",
    "def compute_log_likelihood_mean(neg_log_likelihoods):\n",
    "    \"\"\"Compute the mean of negative log likelihoods for approximate posterior predictive.\"\"\"\n",
    "    \n",
    "    mean_across_models = torch.mean(neg_log_likelihoods, dim=0)\n",
    "    mean_of_neg_log_likelihoods = torch.mean(mean_across_models, dim=1)\n",
    "\n",
    "    return mean_of_neg_log_likelihoods\n",
    "\n",
    "def compute_mean_pointwise_mutual_information(pointwise_mutual_information):\n",
    "    \"\"\"Compute the mean of pointwise mutual information across models.\"\"\"\n",
    "    \n",
    "    mean_across_models = torch.mean(pointwise_mutual_information, dim=0)\n",
    "    return torch.mean(mean_across_models, dim=1)\n",
    "\n",
    "def compute_predictive_entropy(log_likelihoods):\n",
    "    \"\"\"Compute the predictive entropy of approximate posterior predictive.\"\"\"\n",
    "    \n",
    "    num_models = torch.tensor(log_likelihoods.shape[0])\n",
    "    mean_across_models = torch.logsumexp(log_likelihoods, dim=0) - torch.log(num_models)\n",
    "    entropy = -torch.sum(mean_across_models, dim=1) / torch.tensor(mean_across_models.shape[1])\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def compute_predictive_entropy_across_concepts(log_likelihoods, semantic_set_ids):\n",
    "    \"\"\"Compute the semantic entropy across different concepts.\"\"\"\n",
    "    \n",
    "    num_models = torch.tensor(log_likelihoods.shape[0])\n",
    "    mean_across_models = torch.logsumexp(log_likelihoods, dim=0) - torch.log(num_models)\n",
    "    semantic_set_ids = semantic_set_ids[0]  # All models have the same semantic set ids\n",
    "    entropies = []\n",
    "    for row_index in range(mean_across_models.shape[0]):\n",
    "        aggregated_likelihoods = []\n",
    "        row = mean_across_models[row_index]\n",
    "        semantic_ids_row = semantic_set_ids[row_index].to(device)\n",
    "        for semantic_set_id in torch.unique(semantic_ids_row):\n",
    "            aggregated_likelihoods.append(torch.logsumexp(row[semantic_ids_row == semantic_set_id], dim=0))\n",
    "        aggregated_likelihoods = torch.tensor(aggregated_likelihoods) - llh_shift\n",
    "        entropy = - torch.sum(aggregated_likelihoods, dim=0) / torch.tensor(aggregated_likelihoods.shape[0])\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    return torch.tensor(entropies)\n",
    "\n",
    "def compute_margin_probability_uncertainty(log_likelihoods):\n",
    "    \"\"\"Compute margin probability uncertainty measure.\"\"\"\n",
    "    \n",
    "    num_models = torch.tensor(log_likelihoods.shape[0])\n",
    "    mean_across_models = torch.logsumexp(log_likelihoods, dim=0) - torch.log(num_models)\n",
    "    topk_likelihoods, _ = torch.topk(mean_across_models, 2, dim=1, sorted=True)\n",
    "    margin_probabilities = np.exp(topk_likelihoods[:, 0]) - np.exp(topk_likelihoods[:, 1])\n",
    "\n",
    "    return margin_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "list_of_results = []\n",
    "\n",
    "with open('data/likelihoods_st.pkl', 'rb') as infile:\n",
    "    sequences = pickle.load(infile)\n",
    "    list_of_results.append(('gemma', sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall log likelihoods and related metrics\n",
    "log_likelihood_metrics = compute_log_likelihood_metrics(list_of_results)\n",
    "\n",
    "# Compute mutual information from negative log likelihoods\n",
    "mutual_information = compute_mutual_information(-log_likelihood_metrics['neg_log_likelihoods'])\n",
    "\n",
    "# Compute predictive entropy from negative log likelihoods\n",
    "predictive_entropy = compute_predictive_entropy(-log_likelihood_metrics['neg_log_likelihoods'])\n",
    "\n",
    "# Compute predictive entropy over different semantic concepts\n",
    "entropy_across_concepts = compute_predictive_entropy_across_concepts(-log_likelihood_metrics['average_neg_log_likelihoods'],\n",
    "                                                                     log_likelihood_metrics['semantic_set_ids'])\n",
    "\n",
    "# Compute unnormalized entropy over different semantic concepts\n",
    "unnormalized_entropy_across_concepts = compute_predictive_entropy_across_concepts(-log_likelihood_metrics['neg_log_likelihoods'],\n",
    "                                                                                  log_likelihood_metrics['semantic_set_ids'])\n",
    "\n",
    "# Compute margin probability uncertainty measures\n",
    "margin_probabilities = compute_margin_probability_uncertainty(-log_likelihood_metrics['average_neg_log_likelihoods'])\n",
    "unnormalized_margin_probabilities = compute_margin_probability_uncertainty(-log_likelihood_metrics['neg_log_likelihoods'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_elements_per_row(tensor):\n",
    "    \"\"\"Count the number of unique elements in each row of a tensor.\"\"\"\n",
    "    assert len(tensor.shape) == 2\n",
    "    return torch.count_nonzero(torch.sum(torch.nn.functional.one_hot(tensor), dim=1), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique semantic sets\n",
    "num_semantic_sets = count_unique_elements_per_row(log_likelihood_metrics['semantic_set_ids'][0])\n",
    "\n",
    "# Compute average predictive entropy\n",
    "avg_predictive_entropy = compute_predictive_entropy(-log_likelihood_metrics['average_neg_log_likelihoods'])\n",
    "\n",
    "# Initialize lists to store entropy measures on subsets\n",
    "avg_entropy_on_subsets = []\n",
    "entropy_on_subsets = []\n",
    "semantic_entropy_on_subsets = []\n",
    "num_semantic_sets_on_subsets = []\n",
    "num_predictions = log_likelihood_metrics['average_neg_log_likelihoods'].shape[-1]\n",
    "\n",
    "# Compute entropy measures on subsets of predictions\n",
    "for i in range(1, num_predictions + 1):\n",
    "    offset = num_predictions * (i / 100)\n",
    "    avg_entropy_on_subsets.append(compute_predictive_entropy(-log_likelihood_metrics['average_neg_log_likelihoods'][:, :, :int(i)]))\n",
    "    entropy_on_subsets.append(compute_predictive_entropy(-log_likelihood_metrics['neg_log_likelihoods'][:, :, :int(i)]))\n",
    "    semantic_entropy_on_subsets.append(compute_predictive_entropy_across_concepts(-log_likelihood_metrics['average_neg_log_likelihoods'][:, :, :int(i)],\n",
    "                                                                                  log_likelihood_metrics['semantic_set_ids'][:, :, :int(i)]))\n",
    "    num_semantic_sets_on_subsets.append(count_unique_elements_per_row(log_likelihood_metrics['semantic_set_ids'][0][:, :i]))\n",
    "\n",
    "# Compute average pointwise mutual information\n",
    "avg_pointwise_mutual_info = compute_mean_pointwise_mutual_information(log_likelihood_metrics['pointwise_mutual_information'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding computed metrics to the overall results dictionary\n",
    "log_likelihood_metrics['mutual_information'] = mutual_information\n",
    "log_likelihood_metrics['predictive_entropy'] = predictive_entropy\n",
    "log_likelihood_metrics['entropy_across_concepts'] = entropy_across_concepts\n",
    "log_likelihood_metrics['unnormalized_entropy_across_concepts'] = unnormalized_entropy_across_concepts\n",
    "log_likelihood_metrics['num_semantic_sets'] = num_semantic_sets\n",
    "log_likelihood_metrics['margin_probabilities'] = margin_probabilities\n",
    "log_likelihood_metrics['unnormalized_margin_probabilities'] = unnormalized_margin_probabilities\n",
    "log_likelihood_metrics['avg_predictive_entropy'] = avg_predictive_entropy\n",
    "\n",
    "# Adding computed metrics on subsets to the overall results dictionary\n",
    "for i in range(len(avg_entropy_on_subsets)):\n",
    "    log_likelihood_metrics[f'avg_entropy_on_subset_{i + 1}'] = avg_entropy_on_subsets[i]\n",
    "    log_likelihood_metrics[f'entropy_on_subset_{i + 1}'] = entropy_on_subsets[i]\n",
    "    log_likelihood_metrics[f'semantic_entropy_on_subset_{i + 1}'] = semantic_entropy_on_subsets[i]\n",
    "    log_likelihood_metrics[f'num_semantic_sets_on_subset_{i + 1}'] = num_semantic_sets_on_subsets[i]\n",
    "\n",
    "log_likelihood_metrics['avg_pointwise_mutual_info'] = avg_pointwise_mutual_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['neg_log_likelihoods', 'average_neg_log_likelihoods', 'sequence_embeddings', 'pointwise_mutual_information', 'average_neg_log_likelihood_of_most_likely_gen', 'average_neg_log_likelihood_of_second_most_likely_gen', 'neg_log_likelihood_of_most_likely_gen', 'semantic_set_ids', 'id', 'mutual_information', 'predictive_entropy', 'entropy_across_concepts', 'unnormalized_entropy_across_concepts', 'num_semantic_sets', 'margin_probabilities', 'unnormalized_margin_probabilities', 'avg_predictive_entropy', 'avg_entropy_on_subset_1', 'entropy_on_subset_1', 'semantic_entropy_on_subset_1', 'num_semantic_sets_on_subset_1', 'avg_entropy_on_subset_2', 'entropy_on_subset_2', 'semantic_entropy_on_subset_2', 'num_semantic_sets_on_subset_2', 'avg_entropy_on_subset_3', 'entropy_on_subset_3', 'semantic_entropy_on_subset_3', 'num_semantic_sets_on_subset_3', 'avg_entropy_on_subset_4', 'entropy_on_subset_4', 'semantic_entropy_on_subset_4', 'num_semantic_sets_on_subset_4', 'avg_entropy_on_subset_5', 'entropy_on_subset_5', 'semantic_entropy_on_subset_5', 'num_semantic_sets_on_subset_5', 'avg_pointwise_mutual_info'])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(log_likelihood_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/aggregated_likelihoods_generations_st.pkl', 'wb') as outfile:\n",
    "    pickle.dump(log_likelihood_metrics, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin probabilities: tensor([6.5584e-02, 4.4359e-02, 1.4562e-02, 8.6575e-02, 2.0068e-03, 5.8177e-02,\n",
      "        7.2310e-02, 8.6190e-02, 3.8222e-02, 7.4364e-02, 1.2002e-01, 1.7283e-01,\n",
      "        9.3625e-03, 1.9129e-01, 3.0171e-01, 6.9890e-02, 2.2424e-01, 2.5570e-02,\n",
      "        5.4232e-02, 1.4635e-01, 1.6209e-02, 6.7409e-02, 1.9434e-01, 4.0876e-01,\n",
      "        1.7213e-01, 5.5735e-02, 3.9214e-02, 1.6821e-01, 1.0261e-01, 4.2534e-03,\n",
      "        1.7914e-01, 5.3702e-03, 1.5029e-02, 2.4892e-02, 2.0591e-01, 4.4349e-02,\n",
      "        6.3916e-02, 7.1394e-03, 2.6645e-03, 1.0954e-02, 4.4994e-01, 6.6210e-02,\n",
      "        2.9049e-02, 1.8406e-01, 3.7423e-02, 1.8155e-01, 1.1209e-02, 5.8704e-02,\n",
      "        1.8377e-01, 1.8573e-01, 1.1689e-02, 3.5077e-02, 1.7494e-01, 1.6816e-02,\n",
      "        1.6451e-02, 3.2596e-02, 8.6117e-03, 9.6717e-02, 5.1238e-02, 1.9429e-03,\n",
      "        7.0909e-02, 1.2721e-03, 5.5869e-02, 1.8749e-02, 1.1596e-02, 4.5479e-02,\n",
      "        1.6516e-02, 1.0492e-02, 2.5851e-03, 1.2103e-02, 4.7382e-03, 1.3390e-02,\n",
      "        1.0201e-02, 6.7369e-02, 1.6553e-01, 6.4605e-03, 2.6299e-02, 1.0606e-01,\n",
      "        3.4729e-02, 1.2814e-01, 9.2275e-02, 3.9003e-03, 1.5378e-03, 1.3859e-01,\n",
      "        9.4408e-03, 2.1453e-02, 7.8526e-03, 1.1962e-01, 1.8992e-02, 1.2884e-02,\n",
      "        2.2597e-02, 3.1553e-01, 2.7013e-02, 3.0233e-02, 6.4581e-02, 4.3482e-02,\n",
      "        3.7337e-02, 3.3499e-03, 7.7830e-02, 1.3174e-02, 1.1282e-02, 5.2567e-03,\n",
      "        1.7281e-02, 1.5220e-03, 2.6438e-02, 2.2293e-02, 4.1413e-02, 1.9963e-02,\n",
      "        1.3476e-02, 8.3549e-02, 3.2510e-02, 3.7525e-02, 1.9817e-02, 2.7268e-03,\n",
      "        1.0255e-01, 2.6812e-02, 5.2425e-03, 7.1450e-02, 5.0845e-03, 6.1652e-02,\n",
      "        2.7688e-02, 6.6752e-02, 9.4121e-02, 5.0477e-02, 2.2892e-03, 2.2320e-02,\n",
      "        5.9768e-02, 8.8576e-03, 1.7551e-01, 8.0661e-02, 6.5695e-02, 5.3637e-02,\n",
      "        2.6234e-01, 1.0100e-01, 1.4353e-01, 2.2952e-01, 4.9602e-02, 3.5267e-02,\n",
      "        3.0781e-02, 3.0755e-02, 1.0997e-02, 6.9746e-02, 3.7628e-03, 8.4971e-02,\n",
      "        1.7323e-02, 1.8415e-02, 4.8589e-03, 2.6792e-02, 6.4394e-02, 7.0042e-02,\n",
      "        7.3325e-02, 9.7432e-02, 3.6432e-02, 1.9873e-02, 1.5805e-01, 1.4269e-03,\n",
      "        3.5773e-02, 2.4415e-01, 1.7372e-02, 7.2821e-02, 1.3542e-02, 4.0017e-02,\n",
      "        1.9916e-03, 2.9200e-02, 5.8705e-04, 8.1474e-02, 3.3491e-02, 2.4999e-02,\n",
      "        4.9945e-01, 2.1456e-01, 1.0813e-01, 5.0542e-02, 4.0181e-02, 2.1428e-02,\n",
      "        7.2049e-02, 1.5375e-02, 6.7933e-02, 4.0102e-02, 2.1387e-02, 1.0561e-01,\n",
      "        2.5155e-02, 1.9167e-02, 3.4416e-02, 1.1001e-01, 7.6129e-02, 2.8338e-02,\n",
      "        3.7492e-02, 1.4436e-02, 2.0940e-01, 2.4799e-01, 7.1646e-04, 1.0712e-01,\n",
      "        2.0189e-02, 1.0128e-01, 6.4549e-03, 5.7997e-03, 8.6692e-04, 4.0221e-02,\n",
      "        1.5850e-02, 6.7020e-02, 5.0285e-03, 5.7383e-02, 7.6721e-03, 1.9456e-02,\n",
      "        3.9272e-02, 1.2613e-01, 1.1318e-01, 3.2336e-02, 3.7040e-02, 2.0013e-01,\n",
      "        1.3684e-02, 2.5071e-01, 1.2069e-02, 1.2772e-01, 3.1594e-02, 6.8620e-02,\n",
      "        1.9953e-02, 1.7653e-03, 5.6733e-02, 8.2900e-02, 2.1334e-02, 4.6423e-02,\n",
      "        3.9364e-01, 1.8223e-02, 1.2069e-01, 3.1923e-01, 9.1830e-03, 1.6765e-01,\n",
      "        2.0234e-02, 3.0589e-03, 1.1795e-01, 1.4184e-02, 1.3629e-01, 6.7986e-02,\n",
      "        6.4434e-02, 4.0079e-02, 2.9211e-03, 2.2195e-03, 2.3029e-02, 2.8720e-02,\n",
      "        1.5556e-02, 8.6240e-02, 1.0589e-02, 5.8328e-02, 1.2987e-01, 4.5741e-02,\n",
      "        3.2690e-02, 7.5463e-02, 7.5393e-02, 3.2086e-02, 2.3357e-02, 2.3172e-02,\n",
      "        2.8245e-02, 1.8331e-02, 1.9219e-02, 3.4393e-02, 1.5291e-02, 7.0883e-02,\n",
      "        6.9262e-02, 4.2240e-02, 1.2719e-01, 8.8917e-03, 1.6503e-02, 3.9163e-02,\n",
      "        1.3872e-02, 1.1087e-03, 1.8510e-02, 2.5447e-03, 8.7792e-03, 3.9094e-02,\n",
      "        5.9108e-02, 3.2278e-03, 1.4229e-02, 3.3146e-02, 7.6147e-02, 3.3806e-02,\n",
      "        1.3455e-01, 1.0333e-01, 1.7498e-03, 4.1602e-02, 1.8791e-02, 2.7074e-02,\n",
      "        3.5879e-04, 1.6585e-01, 8.8835e-02, 4.3409e-02, 6.4273e-03, 1.1644e-01,\n",
      "        1.1843e-01, 8.4558e-02, 1.6266e-02, 3.4936e-02, 1.4837e-03, 1.2782e-01,\n",
      "        2.7585e-02, 2.3213e-02, 3.7275e-01, 3.1577e-02, 3.0640e-02, 7.4144e-02,\n",
      "        5.6410e-03, 2.6982e-02, 1.4523e-01, 5.6135e-02, 1.0953e-01, 3.4055e-02,\n",
      "        1.3679e-01, 6.7380e-02, 1.6315e-01, 2.1812e-02, 6.5726e-02, 3.9296e-02,\n",
      "        9.7109e-02, 3.1691e-02, 6.7124e-02, 2.0156e-02, 2.8537e-02, 2.2758e-02,\n",
      "        4.0601e-01, 3.0311e-02, 3.2309e-02, 2.2906e-02, 3.2746e-02, 8.6888e-02,\n",
      "        1.4042e-02, 1.8277e-01, 8.2833e-03, 1.4382e-02, 3.4842e-02, 8.4912e-02,\n",
      "        1.4680e-03, 6.3251e-02, 1.1669e-01, 4.2750e-02, 7.4158e-02, 1.0624e-01,\n",
      "        2.4159e-02, 4.1428e-02, 6.2227e-02, 8.2635e-03, 2.2562e-02, 5.1265e-02,\n",
      "        1.2840e-01, 3.7201e-02, 3.7916e-02, 1.6954e-02, 7.0167e-02, 3.0521e-03,\n",
      "        2.2895e-02, 3.4758e-01, 3.8616e-02, 8.9155e-02, 6.1066e-02, 3.7173e-02,\n",
      "        3.9225e-02, 6.2180e-02, 5.1221e-02, 4.2545e-03, 3.6214e-02, 6.5175e-02,\n",
      "        1.0908e-01, 1.0302e-02, 1.5528e-02, 1.1770e-01, 1.1804e-03, 1.5640e-02,\n",
      "        2.1729e-03, 9.9544e-03, 2.2117e-02, 1.1627e-01, 2.8628e-04, 1.4052e-02,\n",
      "        4.5760e-02, 1.1822e-01, 4.3208e-02, 1.3941e-01, 2.5422e-02, 3.9907e-03,\n",
      "        8.1273e-02, 8.7057e-02, 1.4439e-02, 3.5322e-02, 1.5373e-01, 4.6942e-02,\n",
      "        1.3428e-02, 1.1553e-02, 3.4964e-02, 6.2837e-02, 1.6691e-02, 1.4245e-02,\n",
      "        9.6664e-02, 1.4418e-02, 1.0782e-02, 1.2680e-01, 2.3268e-01, 8.5297e-03,\n",
      "        1.9816e-02, 5.2167e-01, 5.6732e-02, 2.4830e-02, 5.6668e-02, 1.2456e-02,\n",
      "        4.6789e-02, 7.1547e-02, 5.2118e-02, 1.9508e-02, 4.3072e-02, 2.6989e-01,\n",
      "        4.0192e-03, 4.7387e-02, 3.6243e-02, 1.0942e-02, 2.5822e-02, 1.8627e-02,\n",
      "        3.7087e-02, 3.4897e-01, 2.2209e-02, 1.0038e-01, 2.6193e-02, 1.7993e-02,\n",
      "        8.1730e-03, 5.1047e-02, 4.2908e-02, 2.7260e-02, 7.3153e-02, 1.6954e-02,\n",
      "        2.1163e-02, 3.8350e-02, 4.2005e-03, 1.1423e-01, 5.7823e-02, 4.4528e-02,\n",
      "        4.4348e-03, 2.1419e-02, 4.4346e-02, 3.6645e-03, 3.7169e-02, 1.0898e-01,\n",
      "        1.2096e-01, 3.9723e-02, 5.6601e-02, 1.1488e-01, 3.8630e-01, 8.5111e-03,\n",
      "        5.8793e-02, 1.6724e-02, 3.1388e-02, 6.7478e-02, 3.6118e-02, 4.6165e-03,\n",
      "        1.1017e-02, 3.7087e-02, 9.4531e-03, 1.1839e-02, 3.7510e-02, 1.9210e-01,\n",
      "        6.0008e-02, 1.7445e-02, 1.0675e-02, 2.2318e-02, 7.7565e-02, 4.5540e-02,\n",
      "        2.1759e-02, 3.0879e-02, 1.2998e-01, 4.4040e-04, 6.1514e-02, 2.4967e-02,\n",
      "        2.9240e-02, 4.1690e-02, 4.6127e-03, 2.1159e-01, 1.0706e-01, 5.5935e-02,\n",
      "        2.6381e-02, 4.9474e-02, 2.3187e-02, 2.7210e-01, 3.2197e-02, 2.9691e-02,\n",
      "        6.2554e-02, 3.7013e-03, 1.3312e-01, 2.8293e-02, 6.4269e-03, 1.0821e-02,\n",
      "        9.8718e-02, 4.9664e-02, 4.3458e-02, 3.5337e-02, 1.2365e-01, 4.6697e-04,\n",
      "        2.6275e-02, 4.5491e-02, 9.8612e-03, 1.8213e-02, 1.7443e-02, 5.6200e-02,\n",
      "        1.4207e-01, 1.4330e-02, 4.8110e-02, 1.3003e-01, 1.7111e-01, 1.1841e-01,\n",
      "        2.2916e-02, 3.8036e-02, 1.4370e-01, 3.9047e-02, 1.1054e-01, 7.8038e-03,\n",
      "        5.3876e-02, 1.1866e-01, 3.4785e-02, 6.2271e-02, 2.1614e-02, 1.3092e-02,\n",
      "        2.2787e-02, 1.5024e-02, 5.1227e-02, 5.2434e-02, 6.0414e-03, 6.1841e-02,\n",
      "        3.7311e-02, 2.5975e-01, 3.4384e-02, 6.9518e-02, 1.1981e-02, 8.3719e-02,\n",
      "        3.2922e-02, 5.7121e-02, 4.2509e-02, 1.4096e-05, 1.5611e-02, 7.1552e-02,\n",
      "        7.4470e-02, 2.4825e-02, 1.3005e-01, 1.3705e-02, 4.3005e-02, 7.8393e-02,\n",
      "        1.4491e-02, 1.9327e-01, 6.4356e-02, 1.1111e-02, 3.2583e-02, 4.3367e-02,\n",
      "        2.4778e-01, 1.0723e-01, 2.8170e-02, 2.4877e-02, 5.9585e-03, 1.3149e-01,\n",
      "        3.8621e-02, 7.9532e-03, 4.0842e-02, 4.0619e-02, 3.2500e-01, 8.2904e-03,\n",
      "        6.9550e-02, 4.1973e-02, 6.9258e-04, 9.8748e-02, 6.1051e-02, 5.4434e-02,\n",
      "        4.8678e-02, 5.6309e-02, 1.4484e-01, 4.1632e-02, 1.8608e-01, 1.8273e-01,\n",
      "        3.1403e-02, 7.7881e-02, 2.9141e-02, 5.5115e-02, 2.2495e-02, 3.9340e-01,\n",
      "        8.0979e-03, 8.2898e-02, 4.1844e-03, 4.7680e-02, 1.7178e-02, 1.1516e-03,\n",
      "        4.5974e-02, 4.9607e-02, 2.0607e-02, 1.1023e-01, 6.5382e-03, 4.5917e-02,\n",
      "        1.1324e-02, 2.5774e-02, 3.4833e-02, 1.9802e-02, 2.7224e-02, 2.4923e-02,\n",
      "        1.5026e-02, 2.5313e-03, 1.4123e-03, 1.5765e-02, 3.1696e-02, 5.7663e-02,\n",
      "        2.6714e-02, 1.3892e-02, 2.4229e-02, 5.3484e-02, 9.4900e-03, 7.5451e-02,\n",
      "        5.1778e-02, 2.0693e-02, 8.4751e-03, 5.0297e-03, 6.5160e-03, 7.8588e-03,\n",
      "        5.0626e-03, 5.4388e-03, 6.0833e-02, 5.3225e-02, 1.4212e-02, 6.0956e-03,\n",
      "        8.0252e-02, 1.0728e-01, 2.5504e-02, 1.0077e-02, 1.1353e-02, 3.6370e-03,\n",
      "        1.7667e-02, 9.7881e-03, 7.0199e-02, 8.7485e-03, 3.0702e-03, 6.9103e-02,\n",
      "        8.5200e-02, 1.1530e-01, 1.9863e-02, 3.2739e-03, 3.6647e-02, 6.9435e-02,\n",
      "        1.1852e-01, 1.8461e-02, 1.0534e-01, 3.8134e-02, 7.2562e-02, 2.8476e-02,\n",
      "        3.0924e-02, 2.9975e-02, 1.0410e-01, 1.0243e-02, 1.1642e-01, 2.2393e-02,\n",
      "        2.9298e-01, 4.8792e-02, 7.4789e-05, 3.9328e-01, 1.7934e-03, 5.8097e-03,\n",
      "        6.6030e-03, 3.0684e-03, 2.5391e-01, 6.6709e-02, 1.3281e-01, 6.8426e-02,\n",
      "        6.9730e-03, 2.8902e-01, 9.0022e-02, 3.7149e-02, 2.0079e-02, 5.8835e-03,\n",
      "        1.6836e-02, 2.3663e-03, 1.1986e-02, 2.4625e-03, 6.2193e-02, 4.4100e-02,\n",
      "        1.2673e-01, 1.0219e-01, 9.8687e-02, 3.7886e-02, 5.7522e-02, 1.5513e-01,\n",
      "        6.2306e-02, 1.6588e-01, 2.0539e-03, 1.0025e-03, 9.8842e-02, 1.1106e-01,\n",
      "        1.9642e-02, 4.6711e-03, 3.9709e-02, 4.9800e-01, 1.2124e-01, 1.2834e-01,\n",
      "        1.3152e-01, 1.1743e-02, 7.7330e-02, 1.8048e-01, 2.0703e-01, 1.0994e-02,\n",
      "        9.9214e-02, 5.5657e-02, 1.2140e-01, 9.7870e-03, 3.8409e-02, 2.5774e-02,\n",
      "        9.4918e-02, 5.4194e-02, 1.1142e-01, 5.9976e-02, 9.9341e-03, 4.1570e-02,\n",
      "        8.2706e-02, 2.6378e-02, 8.1591e-02, 8.5925e-02, 1.1256e-02, 1.5281e-02,\n",
      "        5.8384e-03, 4.3917e-01, 7.4171e-02, 4.8226e-03, 1.4129e-02, 5.0022e-02,\n",
      "        8.6184e-03, 2.2086e-02, 1.0953e-01, 3.8553e-03, 1.4758e-01, 5.9170e-03,\n",
      "        1.0160e-01, 3.7647e-01, 5.0741e-02, 8.3499e-03, 3.1511e-01, 1.3332e-01,\n",
      "        4.2278e-03, 4.5617e-02, 1.2160e-02, 2.0022e-02, 1.4708e-02, 3.2476e-02,\n",
      "        4.1304e-02, 9.5785e-02, 6.2964e-02, 3.5177e-02, 5.0234e-02, 1.5986e-02,\n",
      "        1.1227e-01, 1.9112e-02, 4.7684e-02, 5.6359e-02, 1.6840e-03, 8.2193e-02,\n",
      "        7.0232e-02, 4.3075e-01, 3.1369e-02, 9.4121e-02, 7.0245e-02, 1.7684e-02,\n",
      "        3.4222e-04, 4.3467e-02, 1.4756e-02, 2.7749e-01, 4.0911e-02, 8.3922e-02,\n",
      "        2.9242e-02, 7.4735e-03, 7.8357e-02, 2.4203e-01, 9.6511e-02, 3.0768e-02,\n",
      "        4.0228e-02, 1.0117e-02, 1.9804e-02, 2.9060e-03, 3.4607e-01, 7.2755e-02,\n",
      "        1.1200e-02, 4.8849e-03, 2.9010e-02, 3.5881e-02, 3.1309e-02, 2.4503e-02,\n",
      "        6.3315e-02, 1.0977e-01, 8.4235e-03, 1.2307e-01, 1.3042e-02, 2.5547e-04,\n",
      "        4.0965e-01, 1.3609e-02, 8.5755e-02, 7.4903e-03, 1.1735e-02, 7.5227e-02,\n",
      "        9.4580e-02, 6.7158e-03, 2.0240e-01, 7.9905e-03, 1.8810e-01, 1.6963e-02,\n",
      "        1.6625e-02, 5.7209e-02, 1.4810e-02, 2.9684e-03, 2.8885e-02, 6.4541e-04,\n",
      "        4.4836e-02, 2.6708e-02, 2.5365e-02, 3.5274e-02, 1.9385e-01, 2.7460e-02,\n",
      "        1.2805e-01, 2.0270e-02, 4.2616e-02, 1.9128e-01, 1.9205e-01, 5.8312e-02,\n",
      "        6.7278e-02])\n",
      "Number of semantic sets: tensor([3, 1, 5, 3, 3, 4, 5, 3, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3], device='cuda:0')\n",
      "predictive entropy shape: torch.Size([817])\n",
      "predictive entropy per concept shape: torch.Size([817])\n",
      "torch.Size([1, 817, 5])\n",
      "5\n",
      "torch.Size([817])\n",
      "average predictive entropy on subsets: 5\n",
      "torch.Size([817])\n",
      "tensor([[[232.8364, 222.8156, 230.9995, 301.8983, 179.4398],\n",
      "         [198.3065, 269.4001, 265.7521, 219.6951, 282.3729],\n",
      "         [240.2564, 301.3055, 226.2457, 332.6493, 293.3483],\n",
      "         ...,\n",
      "         [286.4052, 317.7833, 275.6120, 329.4099, 271.7776],\n",
      "         [307.3423, 316.2759, 288.2046, 304.1837, 280.0047],\n",
      "         [269.3201, 323.4178, 410.3197, 293.8913, 294.6648]]])\n",
      "tensor([6.5584e-02, 4.4359e-02, 1.4562e-02, 8.6575e-02, 2.0068e-03, 5.8177e-02,\n",
      "        7.2310e-02, 8.6190e-02, 3.8222e-02, 7.4364e-02, 1.2002e-01, 1.7283e-01,\n",
      "        9.3625e-03, 1.9129e-01, 3.0171e-01, 6.9890e-02, 2.2424e-01, 2.5570e-02,\n",
      "        5.4232e-02, 1.4635e-01, 1.6209e-02, 6.7409e-02, 1.9434e-01, 4.0876e-01,\n",
      "        1.7213e-01, 5.5735e-02, 3.9214e-02, 1.6821e-01, 1.0261e-01, 4.2534e-03,\n",
      "        1.7914e-01, 5.3702e-03, 1.5029e-02, 2.4892e-02, 2.0591e-01, 4.4349e-02,\n",
      "        6.3916e-02, 7.1394e-03, 2.6645e-03, 1.0954e-02, 4.4994e-01, 6.6210e-02,\n",
      "        2.9049e-02, 1.8406e-01, 3.7423e-02, 1.8155e-01, 1.1209e-02, 5.8704e-02,\n",
      "        1.8377e-01, 1.8573e-01, 1.1689e-02, 3.5077e-02, 1.7494e-01, 1.6816e-02,\n",
      "        1.6451e-02, 3.2596e-02, 8.6117e-03, 9.6717e-02, 5.1238e-02, 1.9429e-03,\n",
      "        7.0909e-02, 1.2721e-03, 5.5869e-02, 1.8749e-02, 1.1596e-02, 4.5479e-02,\n",
      "        1.6516e-02, 1.0492e-02, 2.5851e-03, 1.2103e-02, 4.7382e-03, 1.3390e-02,\n",
      "        1.0201e-02, 6.7369e-02, 1.6553e-01, 6.4605e-03, 2.6299e-02, 1.0606e-01,\n",
      "        3.4729e-02, 1.2814e-01, 9.2275e-02, 3.9003e-03, 1.5378e-03, 1.3859e-01,\n",
      "        9.4408e-03, 2.1453e-02, 7.8526e-03, 1.1962e-01, 1.8992e-02, 1.2884e-02,\n",
      "        2.2597e-02, 3.1553e-01, 2.7013e-02, 3.0233e-02, 6.4581e-02, 4.3482e-02,\n",
      "        3.7337e-02, 3.3499e-03, 7.7830e-02, 1.3174e-02, 1.1282e-02, 5.2567e-03,\n",
      "        1.7281e-02, 1.5220e-03, 2.6438e-02, 2.2293e-02, 4.1413e-02, 1.9963e-02,\n",
      "        1.3476e-02, 8.3549e-02, 3.2510e-02, 3.7525e-02, 1.9817e-02, 2.7268e-03,\n",
      "        1.0255e-01, 2.6812e-02, 5.2425e-03, 7.1450e-02, 5.0845e-03, 6.1652e-02,\n",
      "        2.7688e-02, 6.6752e-02, 9.4121e-02, 5.0477e-02, 2.2892e-03, 2.2320e-02,\n",
      "        5.9768e-02, 8.8576e-03, 1.7551e-01, 8.0661e-02, 6.5695e-02, 5.3637e-02,\n",
      "        2.6234e-01, 1.0100e-01, 1.4353e-01, 2.2952e-01, 4.9602e-02, 3.5267e-02,\n",
      "        3.0781e-02, 3.0755e-02, 1.0997e-02, 6.9746e-02, 3.7628e-03, 8.4971e-02,\n",
      "        1.7323e-02, 1.8415e-02, 4.8589e-03, 2.6792e-02, 6.4394e-02, 7.0042e-02,\n",
      "        7.3325e-02, 9.7432e-02, 3.6432e-02, 1.9873e-02, 1.5805e-01, 1.4269e-03,\n",
      "        3.5773e-02, 2.4415e-01, 1.7372e-02, 7.2821e-02, 1.3542e-02, 4.0017e-02,\n",
      "        1.9916e-03, 2.9200e-02, 5.8705e-04, 8.1474e-02, 3.3491e-02, 2.4999e-02,\n",
      "        4.9945e-01, 2.1456e-01, 1.0813e-01, 5.0542e-02, 4.0181e-02, 2.1428e-02,\n",
      "        7.2049e-02, 1.5375e-02, 6.7933e-02, 4.0102e-02, 2.1387e-02, 1.0561e-01,\n",
      "        2.5155e-02, 1.9167e-02, 3.4416e-02, 1.1001e-01, 7.6129e-02, 2.8338e-02,\n",
      "        3.7492e-02, 1.4436e-02, 2.0940e-01, 2.4799e-01, 7.1646e-04, 1.0712e-01,\n",
      "        2.0189e-02, 1.0128e-01, 6.4549e-03, 5.7997e-03, 8.6692e-04, 4.0221e-02,\n",
      "        1.5850e-02, 6.7020e-02, 5.0285e-03, 5.7383e-02, 7.6721e-03, 1.9456e-02,\n",
      "        3.9272e-02, 1.2613e-01, 1.1318e-01, 3.2336e-02, 3.7040e-02, 2.0013e-01,\n",
      "        1.3684e-02, 2.5071e-01, 1.2069e-02, 1.2772e-01, 3.1594e-02, 6.8620e-02,\n",
      "        1.9953e-02, 1.7653e-03, 5.6733e-02, 8.2900e-02, 2.1334e-02, 4.6423e-02,\n",
      "        3.9364e-01, 1.8223e-02, 1.2069e-01, 3.1923e-01, 9.1830e-03, 1.6765e-01,\n",
      "        2.0234e-02, 3.0589e-03, 1.1795e-01, 1.4184e-02, 1.3629e-01, 6.7986e-02,\n",
      "        6.4434e-02, 4.0079e-02, 2.9211e-03, 2.2195e-03, 2.3029e-02, 2.8720e-02,\n",
      "        1.5556e-02, 8.6240e-02, 1.0589e-02, 5.8328e-02, 1.2987e-01, 4.5741e-02,\n",
      "        3.2690e-02, 7.5463e-02, 7.5393e-02, 3.2086e-02, 2.3357e-02, 2.3172e-02,\n",
      "        2.8245e-02, 1.8331e-02, 1.9219e-02, 3.4393e-02, 1.5291e-02, 7.0883e-02,\n",
      "        6.9262e-02, 4.2240e-02, 1.2719e-01, 8.8917e-03, 1.6503e-02, 3.9163e-02,\n",
      "        1.3872e-02, 1.1087e-03, 1.8510e-02, 2.5447e-03, 8.7792e-03, 3.9094e-02,\n",
      "        5.9108e-02, 3.2278e-03, 1.4229e-02, 3.3146e-02, 7.6147e-02, 3.3806e-02,\n",
      "        1.3455e-01, 1.0333e-01, 1.7498e-03, 4.1602e-02, 1.8791e-02, 2.7074e-02,\n",
      "        3.5879e-04, 1.6585e-01, 8.8835e-02, 4.3409e-02, 6.4273e-03, 1.1644e-01,\n",
      "        1.1843e-01, 8.4558e-02, 1.6266e-02, 3.4936e-02, 1.4837e-03, 1.2782e-01,\n",
      "        2.7585e-02, 2.3213e-02, 3.7275e-01, 3.1577e-02, 3.0640e-02, 7.4144e-02,\n",
      "        5.6410e-03, 2.6982e-02, 1.4523e-01, 5.6135e-02, 1.0953e-01, 3.4055e-02,\n",
      "        1.3679e-01, 6.7380e-02, 1.6315e-01, 2.1812e-02, 6.5726e-02, 3.9296e-02,\n",
      "        9.7109e-02, 3.1691e-02, 6.7124e-02, 2.0156e-02, 2.8537e-02, 2.2758e-02,\n",
      "        4.0601e-01, 3.0311e-02, 3.2309e-02, 2.2906e-02, 3.2746e-02, 8.6888e-02,\n",
      "        1.4042e-02, 1.8277e-01, 8.2833e-03, 1.4382e-02, 3.4842e-02, 8.4912e-02,\n",
      "        1.4680e-03, 6.3251e-02, 1.1669e-01, 4.2750e-02, 7.4158e-02, 1.0624e-01,\n",
      "        2.4159e-02, 4.1428e-02, 6.2227e-02, 8.2635e-03, 2.2562e-02, 5.1265e-02,\n",
      "        1.2840e-01, 3.7201e-02, 3.7916e-02, 1.6954e-02, 7.0167e-02, 3.0521e-03,\n",
      "        2.2895e-02, 3.4758e-01, 3.8616e-02, 8.9155e-02, 6.1066e-02, 3.7173e-02,\n",
      "        3.9225e-02, 6.2180e-02, 5.1221e-02, 4.2545e-03, 3.6214e-02, 6.5175e-02,\n",
      "        1.0908e-01, 1.0302e-02, 1.5528e-02, 1.1770e-01, 1.1804e-03, 1.5640e-02,\n",
      "        2.1729e-03, 9.9544e-03, 2.2117e-02, 1.1627e-01, 2.8628e-04, 1.4052e-02,\n",
      "        4.5760e-02, 1.1822e-01, 4.3208e-02, 1.3941e-01, 2.5422e-02, 3.9907e-03,\n",
      "        8.1273e-02, 8.7057e-02, 1.4439e-02, 3.5322e-02, 1.5373e-01, 4.6942e-02,\n",
      "        1.3428e-02, 1.1553e-02, 3.4964e-02, 6.2837e-02, 1.6691e-02, 1.4245e-02,\n",
      "        9.6664e-02, 1.4418e-02, 1.0782e-02, 1.2680e-01, 2.3268e-01, 8.5297e-03,\n",
      "        1.9816e-02, 5.2167e-01, 5.6732e-02, 2.4830e-02, 5.6668e-02, 1.2456e-02,\n",
      "        4.6789e-02, 7.1547e-02, 5.2118e-02, 1.9508e-02, 4.3072e-02, 2.6989e-01,\n",
      "        4.0192e-03, 4.7387e-02, 3.6243e-02, 1.0942e-02, 2.5822e-02, 1.8627e-02,\n",
      "        3.7087e-02, 3.4897e-01, 2.2209e-02, 1.0038e-01, 2.6193e-02, 1.7993e-02,\n",
      "        8.1730e-03, 5.1047e-02, 4.2908e-02, 2.7260e-02, 7.3153e-02, 1.6954e-02,\n",
      "        2.1163e-02, 3.8350e-02, 4.2005e-03, 1.1423e-01, 5.7823e-02, 4.4528e-02,\n",
      "        4.4348e-03, 2.1419e-02, 4.4346e-02, 3.6645e-03, 3.7169e-02, 1.0898e-01,\n",
      "        1.2096e-01, 3.9723e-02, 5.6601e-02, 1.1488e-01, 3.8630e-01, 8.5111e-03,\n",
      "        5.8793e-02, 1.6724e-02, 3.1388e-02, 6.7478e-02, 3.6118e-02, 4.6165e-03,\n",
      "        1.1017e-02, 3.7087e-02, 9.4531e-03, 1.1839e-02, 3.7510e-02, 1.9210e-01,\n",
      "        6.0008e-02, 1.7445e-02, 1.0675e-02, 2.2318e-02, 7.7565e-02, 4.5540e-02,\n",
      "        2.1759e-02, 3.0879e-02, 1.2998e-01, 4.4040e-04, 6.1514e-02, 2.4967e-02,\n",
      "        2.9240e-02, 4.1690e-02, 4.6127e-03, 2.1159e-01, 1.0706e-01, 5.5935e-02,\n",
      "        2.6381e-02, 4.9474e-02, 2.3187e-02, 2.7210e-01, 3.2197e-02, 2.9691e-02,\n",
      "        6.2554e-02, 3.7013e-03, 1.3312e-01, 2.8293e-02, 6.4269e-03, 1.0821e-02,\n",
      "        9.8718e-02, 4.9664e-02, 4.3458e-02, 3.5337e-02, 1.2365e-01, 4.6697e-04,\n",
      "        2.6275e-02, 4.5491e-02, 9.8612e-03, 1.8213e-02, 1.7443e-02, 5.6200e-02,\n",
      "        1.4207e-01, 1.4330e-02, 4.8110e-02, 1.3003e-01, 1.7111e-01, 1.1841e-01,\n",
      "        2.2916e-02, 3.8036e-02, 1.4370e-01, 3.9047e-02, 1.1054e-01, 7.8038e-03,\n",
      "        5.3876e-02, 1.1866e-01, 3.4785e-02, 6.2271e-02, 2.1614e-02, 1.3092e-02,\n",
      "        2.2787e-02, 1.5024e-02, 5.1227e-02, 5.2434e-02, 6.0414e-03, 6.1841e-02,\n",
      "        3.7311e-02, 2.5975e-01, 3.4384e-02, 6.9518e-02, 1.1981e-02, 8.3719e-02,\n",
      "        3.2922e-02, 5.7121e-02, 4.2509e-02, 1.4096e-05, 1.5611e-02, 7.1552e-02,\n",
      "        7.4470e-02, 2.4825e-02, 1.3005e-01, 1.3705e-02, 4.3005e-02, 7.8393e-02,\n",
      "        1.4491e-02, 1.9327e-01, 6.4356e-02, 1.1111e-02, 3.2583e-02, 4.3367e-02,\n",
      "        2.4778e-01, 1.0723e-01, 2.8170e-02, 2.4877e-02, 5.9585e-03, 1.3149e-01,\n",
      "        3.8621e-02, 7.9532e-03, 4.0842e-02, 4.0619e-02, 3.2500e-01, 8.2904e-03,\n",
      "        6.9550e-02, 4.1973e-02, 6.9258e-04, 9.8748e-02, 6.1051e-02, 5.4434e-02,\n",
      "        4.8678e-02, 5.6309e-02, 1.4484e-01, 4.1632e-02, 1.8608e-01, 1.8273e-01,\n",
      "        3.1403e-02, 7.7881e-02, 2.9141e-02, 5.5115e-02, 2.2495e-02, 3.9340e-01,\n",
      "        8.0979e-03, 8.2898e-02, 4.1844e-03, 4.7680e-02, 1.7178e-02, 1.1516e-03,\n",
      "        4.5974e-02, 4.9607e-02, 2.0607e-02, 1.1023e-01, 6.5382e-03, 4.5917e-02,\n",
      "        1.1324e-02, 2.5774e-02, 3.4833e-02, 1.9802e-02, 2.7224e-02, 2.4923e-02,\n",
      "        1.5026e-02, 2.5313e-03, 1.4123e-03, 1.5765e-02, 3.1696e-02, 5.7663e-02,\n",
      "        2.6714e-02, 1.3892e-02, 2.4229e-02, 5.3484e-02, 9.4900e-03, 7.5451e-02,\n",
      "        5.1778e-02, 2.0693e-02, 8.4751e-03, 5.0297e-03, 6.5160e-03, 7.8588e-03,\n",
      "        5.0626e-03, 5.4388e-03, 6.0833e-02, 5.3225e-02, 1.4212e-02, 6.0956e-03,\n",
      "        8.0252e-02, 1.0728e-01, 2.5504e-02, 1.0077e-02, 1.1353e-02, 3.6370e-03,\n",
      "        1.7667e-02, 9.7881e-03, 7.0199e-02, 8.7485e-03, 3.0702e-03, 6.9103e-02,\n",
      "        8.5200e-02, 1.1530e-01, 1.9863e-02, 3.2739e-03, 3.6647e-02, 6.9435e-02,\n",
      "        1.1852e-01, 1.8461e-02, 1.0534e-01, 3.8134e-02, 7.2562e-02, 2.8476e-02,\n",
      "        3.0924e-02, 2.9975e-02, 1.0410e-01, 1.0243e-02, 1.1642e-01, 2.2393e-02,\n",
      "        2.9298e-01, 4.8792e-02, 7.4789e-05, 3.9328e-01, 1.7934e-03, 5.8097e-03,\n",
      "        6.6030e-03, 3.0684e-03, 2.5391e-01, 6.6709e-02, 1.3281e-01, 6.8426e-02,\n",
      "        6.9730e-03, 2.8902e-01, 9.0022e-02, 3.7149e-02, 2.0079e-02, 5.8835e-03,\n",
      "        1.6836e-02, 2.3663e-03, 1.1986e-02, 2.4625e-03, 6.2193e-02, 4.4100e-02,\n",
      "        1.2673e-01, 1.0219e-01, 9.8687e-02, 3.7886e-02, 5.7522e-02, 1.5513e-01,\n",
      "        6.2306e-02, 1.6588e-01, 2.0539e-03, 1.0025e-03, 9.8842e-02, 1.1106e-01,\n",
      "        1.9642e-02, 4.6711e-03, 3.9709e-02, 4.9800e-01, 1.2124e-01, 1.2834e-01,\n",
      "        1.3152e-01, 1.1743e-02, 7.7330e-02, 1.8048e-01, 2.0703e-01, 1.0994e-02,\n",
      "        9.9214e-02, 5.5657e-02, 1.2140e-01, 9.7870e-03, 3.8409e-02, 2.5774e-02,\n",
      "        9.4918e-02, 5.4194e-02, 1.1142e-01, 5.9976e-02, 9.9341e-03, 4.1570e-02,\n",
      "        8.2706e-02, 2.6378e-02, 8.1591e-02, 8.5925e-02, 1.1256e-02, 1.5281e-02,\n",
      "        5.8384e-03, 4.3917e-01, 7.4171e-02, 4.8226e-03, 1.4129e-02, 5.0022e-02,\n",
      "        8.6184e-03, 2.2086e-02, 1.0953e-01, 3.8553e-03, 1.4758e-01, 5.9170e-03,\n",
      "        1.0160e-01, 3.7647e-01, 5.0741e-02, 8.3499e-03, 3.1511e-01, 1.3332e-01,\n",
      "        4.2278e-03, 4.5617e-02, 1.2160e-02, 2.0022e-02, 1.4708e-02, 3.2476e-02,\n",
      "        4.1304e-02, 9.5785e-02, 6.2964e-02, 3.5177e-02, 5.0234e-02, 1.5986e-02,\n",
      "        1.1227e-01, 1.9112e-02, 4.7684e-02, 5.6359e-02, 1.6840e-03, 8.2193e-02,\n",
      "        7.0232e-02, 4.3075e-01, 3.1369e-02, 9.4121e-02, 7.0245e-02, 1.7684e-02,\n",
      "        3.4222e-04, 4.3467e-02, 1.4756e-02, 2.7749e-01, 4.0911e-02, 8.3922e-02,\n",
      "        2.9242e-02, 7.4735e-03, 7.8357e-02, 2.4203e-01, 9.6511e-02, 3.0768e-02,\n",
      "        4.0228e-02, 1.0117e-02, 1.9804e-02, 2.9060e-03, 3.4607e-01, 7.2755e-02,\n",
      "        1.1200e-02, 4.8849e-03, 2.9010e-02, 3.5881e-02, 3.1309e-02, 2.4503e-02,\n",
      "        6.3315e-02, 1.0977e-01, 8.4235e-03, 1.2307e-01, 1.3042e-02, 2.5547e-04,\n",
      "        4.0965e-01, 1.3609e-02, 8.5755e-02, 7.4903e-03, 1.1735e-02, 7.5227e-02,\n",
      "        9.4580e-02, 6.7158e-03, 2.0240e-01, 7.9905e-03, 1.8810e-01, 1.6963e-02,\n",
      "        1.6625e-02, 5.7209e-02, 1.4810e-02, 2.9684e-03, 2.8885e-02, 6.4541e-04,\n",
      "        4.4836e-02, 2.6708e-02, 2.5365e-02, 3.5274e-02, 1.9385e-01, 2.7460e-02,\n",
      "        1.2805e-01, 2.0270e-02, 4.2616e-02, 1.9128e-01, 1.9205e-01, 5.8312e-02,\n",
      "        6.7278e-02])\n"
     ]
    }
   ],
   "source": [
    "print('Margin probabilities:', margin_probabilities)\n",
    "print('Number of semantic sets:', num_semantic_sets)\n",
    "print('predictive entropy shape:', predictive_entropy.shape)\n",
    "print('predictive entropy per concept shape:', entropy_across_concepts.shape)\n",
    "print(log_likelihood_metrics['average_neg_log_likelihoods'].shape)\n",
    "print(len(num_semantic_sets_on_subsets))\n",
    "print(num_semantic_sets_on_subsets[0].shape)\n",
    "print('average predictive entropy on subsets:', len(avg_entropy_on_subsets))\n",
    "print(avg_entropy_on_subsets[0].shape)\n",
    "print(log_likelihood_metrics['pointwise_mutual_information'])\n",
    "print(log_likelihood_metrics['margin_probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
